{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T18:11:03.423419639Z",
     "start_time": "2023-06-12T18:11:03.117778768Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data_raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_raw: pd.DataFrame = pd.read_parquet(DATA_PATH + \"movies.pq\")  # movies names and genres\n",
    "ratings_raw: pd.DataFrame = pd.read_parquet(DATA_PATH + \"ratings.pq\").drop(columns=\"timestamp\")  # users' ratings\n",
    "tags_raw: pd.DataFrame = pd.read_parquet(DATA_PATH + \"tags.pq\").drop(columns=\"timestamp\")  # users' tags for movies\n",
    "links_raw: pd.DataFrame = pd.read_parquet(DATA_PATH + \"links.pq\")  # ids from different datasets\n",
    "tmdb_movies_raw: pd.DataFrame = pd.read_parquet(DATA_PATH + \"tmdb_5000_movies.pq\")  # tmdb movies data\n",
    "tmdb_credits_raw: pd.DataFrame = pd.read_parquet(DATA_PATH + \"tmdb_5000_credits.pq\")  # tmdb movies' creators data\n",
    "genome_data: pd.DataFrame = pd.read_parquet(DATA_PATH + \"genome.pq\")  # movies' tags and relevance of those"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMDB preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_movies: pd.DataFrame = tmdb_movies_raw[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"title\",\n",
    "        \"genres\",\n",
    "        \"release_date\",\n",
    "        \"production_countries\",\n",
    "        \"runtime\",\n",
    "        \"revenue\",\n",
    "        \"popularity\",\n",
    "        \"vote_average\",\n",
    "        \"vote_count\",\n",
    "    ]\n",
    "]\n",
    "tmdb_movies.loc[:, \"genres\"] = tmdb_movies.genres.transform(lambda y: json.loads(y, object_hook=lambda x: x[\"name\"]))\n",
    "tmdb_movies.loc[:, \"production_countries\"] = tmdb_movies.production_countries.transform(\n",
    "    lambda y: json.loads(y, object_hook=lambda x: x[\"name\"])\n",
    ")\n",
    "tmdb_movies.loc[:, \"release_date\"] = tmdb_movies.release_date.transform(lambda x: str(x).split(\"-\", maxsplit=1)[0])\n",
    "tmdb_movies = tmdb_movies.rename(columns={\"id\": \"movie_id\", \"release_date\": \"year\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_credits: pd.DataFrame = tmdb_credits_raw[[\"movie_id\", \"cast\", \"crew\"]]\n",
    "tmdb_credits.loc[:, \"cast\"] = tmdb_credits.cast.transform(lambda y: json.loads(y, object_hook=lambda x: x[\"name\"])[:10])\n",
    "tmdb_credits.loc[:, \"crew\"] = tmdb_credits.crew.transform(json.loads)\n",
    "tmdb_credits.loc[:, \"crew\"] = tmdb_credits.crew.transform(lambda x: [el for el in x if el[\"job\"] == \"Director\"])\n",
    "tmdb_credits = tmdb_credits[tmdb_credits.crew.apply(len) >= 1]\n",
    "tmdb_credits.loc[:, \"crew\"] = tmdb_credits.crew.transform(lambda x: [el[\"name\"] for el in x])\n",
    "tmdb_credits = tmdb_credits.rename(columns={\"crew\": \"director\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_data: pd.DataFrame = tmdb_movies.merge(tmdb_credits, on=\"movie_id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies: pd.DataFrame = movies_raw.merge(links_raw, on=\"movieId\")[[\"movieId\", \"tmdbId\", \"title\", \"genres\"]]\n",
    "movies = movies.merge(tmdb_movies, left_on=\"tmdbId\", right_on=\"movie_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions from https://towardsdatascience.com/movielens-1m-deep-dive-part-i-8acfeda1ad4\n",
    "\n",
    "\n",
    "def get_wikipedia_page_name(movie_title: str) -> str:\n",
    "    matching_pages: list[str] = wikipedia.search(movie_title)\n",
    "    if len(matching_pages) == 0:\n",
    "        return \"\"\n",
    "    return matching_pages[0]\n",
    "\n",
    "\n",
    "def get_movie_plot(page_name: str) -> Optional[str]:\n",
    "    try:\n",
    "        try:\n",
    "            movie_page_content = str(wikipedia.page(page_name, auto_suggest=False).content)\n",
    "        except wikipedia.DisambiguationError as disamberror:\n",
    "            for option in disamberror.options:\n",
    "                if \"film\" in option:\n",
    "                    movie_page_content = str(wikipedia.page(option, auto_suggest=False).content)\n",
    "            return None\n",
    "    except (wikipedia.PageError, KeyError):\n",
    "        return None\n",
    "    re_groups = re.search(\"Plot ==(.*?)=+ [A-Z]\", str(movie_page_content).replace(\"\\n\", \"\"))\n",
    "    if re_groups:\n",
    "        return re_groups.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[\"wikipedia_page_name\"] = movies[\"title_x\"].progress_apply(get_wikipedia_page_name)\n",
    "movies[\"movie_plot\"] = movies[\"wikipedia_page_name\"].progress_apply(get_movie_plot)\n",
    "print(f'There are {movies[\"movie_plot\"].isna().sum()} NaN movie plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.drop(columns=\"title_x\").rename(columns={\"title_y\": \"title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc[:, \"genres_x\"] = movies.genres_x.apply(lambda x: x.split(\"|\"))\n",
    "movies.loc[:, \"genres_y\"] = movies.genres_y.apply(list)\n",
    "movies[\"genres\"] = (movies.genres_x + movies.genres_y).apply(set).apply(list)\n",
    "movies.drop(columns=[\"genres_x\", \"genres_y\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_no_genres(genres_list: list[str]) -> list[str]:\n",
    "    if (\"(no genres listed)\" in genres_list) and len(genres_list) > 1:\n",
    "        return [genre for genre in genres_list if genre != \"(no genres listed)\"]\n",
    "    return genres_list\n",
    "\n",
    "\n",
    "def remove_foreign(genres_list: list[str]) -> list[str]:\n",
    "    if \"Foreign\" in genres_list:\n",
    "        if len(genres_list) > 1:\n",
    "            return [genre for genre in genres_list if genre != \"Foreign\"]\n",
    "        return [\"(no genres listed)\"]\n",
    "    return genres_list\n",
    "\n",
    "\n",
    "movies.loc[:, \"genres\"] = movies.genres.apply(remove_no_genres).apply(remove_foreign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_music(genres_list: list[str]) -> list[str]:\n",
    "    if \"Music\" in genres_list:\n",
    "        tmp = [genre for genre in genres_list if genre != \"Music\"]\n",
    "        if \"Musical\" in genres_list:\n",
    "            return tmp\n",
    "        return tmp + [\"Musical\"]\n",
    "    return genres_list\n",
    "\n",
    "\n",
    "def remove_duplicate_scifi(genres_list: list[str]) -> list[str]:\n",
    "    if \"Science Fiction\" in genres_list:\n",
    "        tmp = [genre for genre in genres_list if genre != \"Science Fiction\"]\n",
    "        if \"Sci-Fi\" in genres_list:\n",
    "            return tmp\n",
    "        return tmp + [\"Sci-Fi\"]\n",
    "    return genres_list\n",
    "\n",
    "\n",
    "movies.loc[:, \"genres\"] = movies.genres.apply(remove_duplicate_music).apply(remove_duplicate_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_country = tmdb_data[[\"movie_id\", \"production_countries\"]]\n",
    "movies = movies.merge(production_country, left_on=\"tmdbId\", right_on=\"movie_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.drop(columns=[\"wikipedia_page_name\", \"movie_id_y\"]).rename(columns={\"movie_id_x\": \"movie_id\"})\n",
    "movies = movies.merge(tmdb_data, left_on=\"movieId\", right_on=\"movie_id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сводим фильмы с тегами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RELEVANCE_THRESHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies_raw.merge(genome_data, on=\"movieId\")\n",
    "movies = movies[movies.relevance > TAG_RELEVANCE_THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = pd.Series(movies.tag.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_tags = movies.tag.apply(stemmer.stem)\n",
    "unique_stammed = stemmed_tags.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "tags_with_pos = all_tags.apply(str.split).apply(nltk.pos_tag)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается много пересекающихся тегов, в том числе потому, что многие похожие по смыслу теги записываются в несколько слов, с ошибками и т.д. Надо придумать, че с этим сделать  \n",
    "Как идея -- попробовать посчитать встречаемость слов (может быть, до и после стемминга?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list = list(map(lambda tags: tags.split(), list(all_tags)))  # type: ignore [no-any-return]\n",
    "words = []\n",
    "for tags in tags_list:\n",
    "    words += tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words, counter_words = np.unique(np.array(words), return_counts=True)\n",
    "word_counts = dict(zip(unique_words, counter_words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно, время выставления также стоит учитывать, т.к. тогда мы сможем каким-то образом учитывать изменение в предпочтениях пользователя. Хотя, с другой стороны, для mvp нам это явно не поможет (это скорее улучшение для существующих пользователей). В общем, пока выкинем, потом придумаем"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
